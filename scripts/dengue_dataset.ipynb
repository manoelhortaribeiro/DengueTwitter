{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creates Geo-Spatial Dengue Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cd /home/manoelribeiro/PycharmProjects/GeoDiseaseTwitter/\n",
    "from scripts.dengue_dataset_utils import str_to_indexes, load_data, match_all, treat_data_nn, path_leaf\n",
    "from keras.models import load_model\n",
    "from nnet.data_utils import Data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import json\n",
    "import glob\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"id_tweet\", \"timestamp\", \"geolocation\", \"id_user\", \"n_friends\", \"n_followers\", \"text\", \"reg1\", \"reg2\", \"state\", \"city\"]\n",
    "keywords = [\"dengue\", \"aedes\"]\n",
    "model = load_model(\"./nnet/logs/backup_model_best.hdf5\")\n",
    "config = json.load(open(\"./nnet/dengue.json\"))\n",
    "labels = [\"Campaign\", \"Personal\", \"Information\", \"Opinion\", \"Joke\"]\n",
    "labels_dict = {\"c\":\"Campaign\", \"e\":\"Personal\", \"i\":\"Information\", \"o\":\"Opinion\", \"p\":\"Joke\"}\n",
    "\n",
    "dict_alphabet = {}\n",
    "\n",
    "for idx, char in enumerate(config[\"data\"][\"alphabet\"]):\n",
    "    dict_alphabet[char] = idx + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Makes processed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ CHANGE THIS ############\n",
    "start = 0\n",
    "do_it = False\n",
    "do_text = False\n",
    "do_nn = False\n",
    "make_check = False\n",
    "do_incorporate = True\n",
    "#####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make city files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = -1\n",
    "\n",
    "if do_it:\n",
    "    for city in glob.glob(\"./data/cidades_total/*\"):\n",
    "        count+=1\n",
    "        if count < start:\n",
    "            continue\n",
    "            \n",
    "        # Loads the data\n",
    "        df = load_data(city)\n",
    "\n",
    "        # Filters tweets by keyword matching\n",
    "        df.loc[:, \"keyword\"] = df.text.apply(lambda x: match_all(x, keywords)).values\n",
    "\n",
    "        # Drop column \"state\"\n",
    "        df = df.drop(\"state\", axis=1)\n",
    "\n",
    "        # Create \"user_keywords\" field\n",
    "        user_keywords = df.groupby(\"id_user\")[\"keyword\"].aggregate(lambda x: np.any(x.values))\n",
    "        user_keywords = user_keywords[user_keywords]\n",
    "        df.loc[:, \"user_keywords\"] = df.apply(lambda x: x[\"id_user\"] in user_keywords, axis=1 )\n",
    "        \n",
    "        # print(city, (df[\"user_keywords\"] == True).sum())\n",
    "        \n",
    "        # Creates new files with neural networks\n",
    "        if (df[\"user_keywords\"] == True).sum() > 0  and do_nn:\n",
    "            inputs = list(df.loc[df[\"keyword\"], \"text\"].apply(lambda x: list(treat_data_nn(x, config[\"data\"][\"input_size\"], dict_alphabet))).values)\n",
    "            inputs = np.asarray(inputs, dtype='int64')\n",
    "            y_score = model.predict(inputs)\n",
    "            mask = y_score.max(axis=1,keepdims=1) == y_score\n",
    "            df.loc[:, \"kw_type\"] = \"Other\"\n",
    "            df.loc[df[\"keyword\"], \"kw_type\"] = [labels[i] for i in mask.argmax(axis=1)]\n",
    "            df.to_csv(\"./data/cities_processed/\" + path_leaf(city) + \".csv\")\n",
    "        \n",
    "        # Writes cities and number of relevant tweets into a file!\n",
    "        if do_text:\n",
    "            with open(\"./data/city_count\", \"a\") as f:\n",
    "                f.write(\"{0},{1}\\n\".format(city, (df[\"user_keywords\"] == True).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make file to be manually annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if make_check:\n",
    "    df_all = pd.DataFrame(columns=[\"kw_type\",\"text\", \"city\"])\n",
    "    for city in glob.glob(\"./data/cities_processed/*\"):\n",
    "        df = pd.read_csv(city, index_col=0)\n",
    "        df.drop(df[df.kw_type == \"Other\"].index.values, axis=0, inplace=True)\n",
    "        df.loc[:, \"city\"] = city\n",
    "        df_all = pd.concat([df_all, df], sort=False)    \n",
    "    df_all.sort_values([\"kw_type\", \"text\"], axis=0).to_csv(\"./data/_all_check.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Incorporates manual annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = set()\n",
    "if do_incorporate:\n",
    "    df_all = pd.read_csv(\"./data/all_check.csv\", index_col=0)\n",
    "    for city in glob.glob(\"./data/cities_processed/*\"):\n",
    "        df = pd.read_csv(city, index_col=0)\n",
    "        users = users.union(set(list(df.id_user.values)))\n",
    "        df_all_city = df_all.loc[df_all.city == city, :]\n",
    "        df.loc[df_all_city.index.values, \"kw_type\"] = df_all_city[\"kw_type\"]\n",
    "        # Drop column \"state\"\n",
    "        df = df.drop(\"text\", axis=1)\n",
    "        df.to_csv(\"./data/cities/\" + path_leaf(city) )\n",
    "print(len(users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Final Adjustments to Key-Word Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/dengue/classificados_all_header.tsv\", sep=\"\\t\")\n",
    "df = df.rename(columns={\"class\": \"kw_type\", \"id\": \"tweet_id\", \"date\":\"timestamp\"})\n",
    "df.loc[:, \"kw_type\"] = df.kw_type.apply(lambda x: labels_dict[x])\n",
    "df = df.drop(\"text\", axis=1)\n",
    "df.loc[:, \"timestamp\"] = df.timestamp.apply(lambda s:  str(int(time.mktime(datetime.datetime.strptime(s.strip(), \"%Y-%m-%d\").timetuple()))))\n",
    "df.to_csv(\"./data/keywords_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots number of tweets of interest per city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = pd.read_csv(\"./data/city_count\").sort_values(by=\"number\", ascending=False).reset_index(drop=True)\n",
    "cities[\"numeric\"] = cities.index\n",
    "ax = cities.plot(x=\"numeric\", y=\"number\", kind=\"scatter\", figsize=(4.5,4), xticks = list(range(0,301, 30)), xlim=[0,300], logy=True, \n",
    "                yticks=[10**1, 10**2, 10**3, 10**4, 10**5, 10**6], alpha=0.8)\n",
    "ax.set_title(\"Number of Tweets of Interest per City\")\n",
    "ax.set_xlabel(\"i-th city\")\n",
    "ax.set_ylabel(\"#tweets of interest\")\n",
    "print(\"Total number of cities:\", len(cities))\n",
    "print(\"Number of cities with any tweet of interest\", (cities[\"number\"] > 0).sum())\n",
    "fig = ax.get_figure()\n",
    "fig.savefig('./data/full_figure.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gets prevalence in the labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Keyword Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv(\"./data/dengue/classificados_all_header.tsv\", sep=\"\\t\", )\n",
    "\n",
    "tmp = dict(Counter(df[\"class\"].values))\n",
    "tmp_p = dict()\n",
    "total = 0\n",
    "for key, val in tmp.items():\n",
    "    total += val\n",
    "for key, val in tmp.items():\n",
    "    tmp_p[key] = tmp[key]/total\n",
    "    \n",
    "print(tmp)\n",
    "print(tmp_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Geolocated Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv(\"./data/all_check.csv\")\n",
    "\n",
    "tmp = dict(Counter(df[\"kw_type\"].values))\n",
    "tmp_p = dict()\n",
    "total = 0\n",
    "for key, val in tmp.items():\n",
    "    total += val\n",
    "for key, val in tmp.items():\n",
    "    tmp_p[key] = tmp[key]/total\n",
    "    \n",
    "print(tmp)\n",
    "print(tmp_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "neural",
   "language": "python",
   "name": "neural"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}